
#' Runs mixed logit estimation
#'
#' Using inputs generated by `prepare_mxl_data()`, run likelihood maximization for mixed logit.
#'
#' @param input_data List output from `prepare_mxl_data()`
#' @param eta_draws Array of shape K_x x S x N with standard normal draws for random coefficients
#' @param rc_correlation Logical indicating whether to estimate full covariance matrix for random coefficients
#' @param use_asc Logical indicating whether to include alternative-specific constants
#' @param theta_init Initial parameter vector for optimization. If NULL, a zero vector of appropriate length is used.
#' @param nloptr_opts List of options to pass to `nloptr::nloptr()`. If NULL, default options are used.
#' @return Result object from `nloptr::nloptr()`, with an added `alt_mapping` element.
#' @importFrom nloptr nloptr
#' @export
run_mxlogit <- function(
    input_data,
    eta_draws,
    rc_correlation,
    use_asc = TRUE,
    theta_init = NULL,
    nloptr_opts = NULL
) {

  # Initial parameter vector theta_init
  if (is.null(theta_init)) {
    J <- nrow(input_data$alt_mapping)
    K_x <- ncol(input_data$X)
    K_w <- ncol(input_data$W)
    L_size <- if (input_data$rc_correlation) {K_w * (K_w + 1) / 2} else {K_w}
    theta_init <- rep(0, K_x + L_size + J - 1)
  }

  # Optimization options (default)
  if (is.null(nloptr_opts)) {
    nloptr_opts <- list(
      "algorithm" = "NLOPT_LD_LBFGS",
      "xtol_rel" = 1.0e-8,
      "maxeval" = 1e+3,
      "print_level" = 0L
    )
  }

  # Run the optimization
  time <- system.time({
    result <- nloptr::nloptr(
      x0 = theta_init,
      eval_f = mxl_loglik_gradient_parallel,
      opts = nloptr_opts,
      X = input_data$X,
      W = input_data$W,
      alt_idx = input_data$alt_idx,
      choice_idx = input_data$choice_idx,
      M = input_data$M,
      weights = input_data$weights,
      rc_correlation = rc_correlation,
      eta_draws = eta_draws,
      use_asc = use_asc,
      include_outside_option = input_data$include_outside_option
    )
  })

  cat("Optimization run time", convertTime(time), "\n\n")
  print(result)
  result$alt_mapping <- input_data$alt_mapping
  return(result)
}


#' Prepare inputs for `mxl_loglik_gradient_parallel()`
#'
#' Prepares and validates inputs for mixed logit estimation routine.
#'
#' @param data Data frame containing choice data
#' @param id_col Name of the column identifying choice situations (individuals)
#' @param alt_col Name of the column identifying alternatives
#' @param choice_col Name of the column indicating chosen alternative (1 = chosen, 0 = not chosen)
#' @param covariate_cols Vector of names of columns to be used as covariates
#' @param random_var_cols Vector of names of columns to be used as random variables
#' @param weights Optional vector of weights for each choice situation. If NULL, equal weights are used.
#' @param outside_opt_label Label for the outside option (if any). If NULL, no outside option is assumed.
#' @param include_outside_option Logical indicating whether to include an outside option in the model.
#' @return A list containing prepared inputs for mixed logit estimation.
#' @import data.table
#' @export
prepare_mxl_data <- function(
    data,
    id_col,
    alt_col,
    choice_col,
    covariate_cols,
    random_var_cols,
    weights = NULL,
    outside_opt_label = NULL,
    include_outside_option = FALSE
) {

  ## Preliminary housekeeping --------------------------------------------------
  dt <- as.data.table(data)[]

  # Check if all relevant variables are available
  needed <- c(id_col, alt_col, choice_col, covariate_cols, random_var_cols)
  if (!all(needed %in% names(dt)))
    stop("Missing columns: ",
         paste(setdiff(needed, names(dt)), collapse = ", "))

  # Drop non-relevant variables
  vars_to_drop <- setdiff(names(dt), needed)
  if (length(vars_to_drop) > 0) {
    dt[, (vars_to_drop) := NULL]
  }

  ## Drop ids with missing observations ----------------------------------------
  dt[, HAS_NA := rowSums(is.na(.SD)) > 0]
  ids_to_drop <- dt[HAS_NA==TRUE, get(id_col)] |> unique()
  if (length(ids_to_drop) > 0) {
    dt <- dt[!(get(id_col) %in% ids_to_drop)]
    warning("Removed ", length(ids_to_drop),
            " choice situations containing missing values.")
  }
  if (nrow(dt) == 0) {
    stop("All choice situations removed due to missing values.")
  }
  dt[, HAS_NA := NULL]

  ## Sanity checks ---------------------------------------------------------

  ## covariates must be numeric
  if (!all(vapply(dt[, ..covariate_cols], is.numeric, logical(1L))))
    stop("All covariates must be numeric.")
  if (!all(vapply(dt[, ..random_var_cols], is.numeric, logical(1L))))
    stop("All covariates must be numeric.")

  ## choice column must be 0 or 1
  bad_choice <- dt[[choice_col]] %in% c(0, 1) == FALSE
  if (any(bad_choice))
    stop("`", choice_col, "` must contain only 0 and 1.")

  ## Exactly one '1' per choice situation
  by_id <- dt[, .(chosen = sum(get(choice_col))), by = id_col]
  if (include_outside_option == FALSE && any(by_id$chosen != 1)) {
    stop("Each ", id_col, " must have exactly one chosen alternative (one '1' in ",
         choice_col, ").")
  }
  if (include_outside_option && any(by_id$chosen > 1)) {
    stop("Each ", id_col, " must have at most one chosen alternative (one '1' in ",
         choice_col, "). An id with no explicit choice is assumed to be outside option.")
  }

  ## Create integer alternative codes ------------------------------------------

  if (!is.null(outside_opt_label) && include_outside_option==FALSE) {
    levels <- c(outside_opt_label, sort(setdiff(unique(dt[[alt_col]]), outside_opt_label)))
  } else {
    levels <- sort(unique(dt[[alt_col]]))
  }

  dt[, alt_int := as.integer(factor(get(alt_col), levels = levels))]

  ## Order rows ----------------------------------------------------------------
  ##   within each id: ascending alternative id
  ##   between ids   : ascending id
  setorderv(dt, c(id_col, "alt_int"))

  ## index of each row within its choice set
  dt[, idx_in_group := seq_len(.N), by = id_col]

  ## Build objects -------------------------------------------------------------
  ## design matrix
  X <- as.matrix(dt[, ..covariate_cols])
  X <- check_collinearity(X)

  W <- as.matrix(dt[, ..random_var_cols])
  W <- check_collinearity(W)

  cols_to_drop <- union(covariate_cols, random_var_cols)
  dt[, (cols_to_drop) := NULL]

  ## alternative ids used for delta coefficients
  alt_idx <- as.integer(dt$alt_int)                             # length == sum(M)

  ## M[i] - # alternatives per choice situation
  M <- dt[, .N, by = id_col][["N"]]                             # length N

  ## N: number of individuals / choice situations
  ids <- dt[, get(id_col)][!duplicated(dt[[id_col]])]  # vector of ids in *current* order
  N   <- length(ids)

  ## choice_idx[i] - 1-based index *within* the choice set data
  ## 0 == outside option (only if chosen = 0 for all inside options & include_outside_option == TRUE)
  if (include_outside_option) {
    # start with all-zero (everyone assumed to pick the outside good)
    choice_idx <- integer(N)
    chosen_dt <- dt[get(choice_col) == 1, .(pos = idx_in_group), by = id_col]

    # match chosen ids back to the master index vector
    setkeyv(chosen_dt, id_col)
    choice_idx[match(chosen_dt[[id_col]], ids)] <- chosen_dt$pos
  } else {
    # exactly one explicit choice per id
    choice_idx <- dt[get(choice_col) == 1, idx_in_group]
  }

  # Weights default = 1
  if (is.null(weights)) weights <- rep(1, N)

  ## Alternative summary -------------------------------------------------------

  if (include_outside_option) {
    inside_alt_mapping <- dt[
      , .(N_OBS = .N, N_CHOICES = sum(get(choice_col))),
      keyby = c("alt_int", alt_col)
    ]
    outside_alt_mapping <- data.table(alt_int=0L, N_OBS = N, N_CHOICES = sum(choice_idx == 0L))
    outside_alt_mapping[[alt_col]] <- outside_opt_label
    alt_mapping <- list(outside_alt_mapping, inside_alt_mapping) |>
      rbindlist(use.names = TRUE, fill = TRUE)
    setcolorder(alt_mapping, c("alt_int", alt_col, "N_OBS", "N_CHOICES"))
  } else {
    alt_mapping <- dt[
      , .(N_OBS = .N, N_CHOICES = sum(get(choice_col))),
      keyby = c("alt_int", alt_col)
    ]
  }

  alt_mapping[, `:=`(
    TAKE_RATE = N_CHOICES / N_OBS,
    MKT_SHARE = N_CHOICES / sum(N_CHOICES)
  )]

  ## Final validity checks -----------------------------------------------------
  stopifnot(
    length(alt_idx)    == nrow(X),
    length(choice_idx) == N,
    length(M)          == N,
    length(weights)    == N,
    all(is.finite(X)),
    all(is.finite(W))
  )

  ## Return output -------------------------------------------------------------
  list(
    X           = X,
    W           = W,
    alt_idx     = alt_idx,
    choice_idx  = as.integer(choice_idx),
    M           = M,
    N           = N,
    weights     = weights,
    include_outside_option = include_outside_option,
    alt_mapping = alt_mapping
  )
}

#' Halton draws for mixed logit
#'
#' Create halton normal draws in appropriate format for mixed logit estimation
#'
#' @param S Number of draws for each choice situation
#' @param N number of choice situations
#' @param K_x dimension of random coefficients
#' @return K_x x S x N array with halton standard normal draws
#' @importFrom randtoolbox halton
#' @export
get_halton_normals <- function(S, N, K_x) {
  # Initialize the eta_draws array
  eta_draws <- array(0, dim = c(K_x, S, N))

  # Generate Halton draws for each individual
  for (i in 1:N) {
    # Use a different starting index for each individual to reduce correlation
    start_index <- (i - 1) * S + 1

    # Generate Halton sequences
    halton_seq <- randtoolbox::halton(n = S, dim = K_x, start = start_index, normal = TRUE)

    # Store in eta_draws
    eta_draws[, , i] <- t(halton_seq)  # Transpose to match dimensions (K_x x S)
  }
  return(eta_draws)
}

convertTime <- function(time) {
  et <- time["elapsed"]
  if (et < 1) {
    s <- round(et, 2)
  } else {
    s <- round(et, 0)
  }
  h <- s %/% 3600
  s <- s - 3600 * h
  m <- s %/% 60
  s <- s - 60 * m
  return(paste(h, "h:", m, "m:", s, "s", sep = ""))
}

vech <- function(M) M[lower.tri(M, diag = TRUE)]

#' Coefficient summary table for mixed logit model
#'
#' Prints and saves coefficient summary table for mixed logit model
#'
#' @param opt_result Result object from nloptr optimization containing at least 'solution' element
#' @param X Design matrix used in estimation
#' @param W Design matrix for random coefficients
#' @param alt_idx Alternative indices for each observation  
#' @param choice_idx Chosen alternative indices for each choice situation
#' @param M Vector of number of alternatives per choice situation
#' @param weights Vector of weights for each choice situation
#' @param use_asc Logical indicating whether ASCs were included in the model
#' @param include_outside_option Logical indicating whether an outside option was included in the model
#' @param omit_asc_output Logical indicating whether to omit ASC parameters from the output table
#' @param param_names Optional vector of parameter names. If NULL, default names are generated.
#' @param file_name Optional file path to save the coefficient summary table as a CSV file. If NULL, no file is saved.
#' @importFrom stats pnorm
#' @export
get_mxl_result <- function(
    opt_result,
    X, W, alt_idx, choice_idx, M, weights, eta_draws,
    rc_correlation,
    use_asc,
    include_outside_option,
    omit_asc_output = FALSE,
    param_names = NULL,
    file_name = NULL
) {
  # Extract the estimated parameter vector
  if (is.null(opt_result$solution)) {
    stop("opt_result must contain '$solution' with the parameter estimates.")
  }
  est_theta <- opt_result$solution
  p_len <- length(est_theta)

  K_x <- ncol(X)
  K_w <- ncol(W)
  L_size <- if (rc_correlation) {
    K_w * (K_w + 1) / 2
  } else {
    K_w
  }
  delta_length <- length(est_theta) - K_x - L_size

  # Compute Hessian at est_theta
  hess <- mxl_hessian_parallel(
    theta = est_theta,
    X = X,
    W = W,
    alt_idx = alt_idx,
    choice_idx = choice_idx,
    M = M,
    weights = weights,
    eta_draws = eta_draws,
    use_asc = use_asc,
    rc_correlation = rc_correlation,
    include_outside_option = include_outside_option
  )

  # Try to invert Hessian for standard errors
  vcov_mat <- NULL
  se <- rep(NA, p_len)  # default in case of errors

  singular_flag <- FALSE
  tryCatch({
    vcov_mat <- solve(hess)
  }, error = function(e) {
    singular_flag <<- TRUE
    message("Error computing vcov_mat (likely singular Hessian): ", e$message)
  })

  # Construct SEs for Random Coefficient var-cov matrix (Sigma)
  L_idx_start <- K_x + 1
  L_idx_end   <- K_x + L_size
  L_params_hat <- est_theta[L_idx_start:L_idx_end]
  Sigma_hat <- build_var_mat(L_params_hat, K_w, rc_correlation)

  if (rc_correlation) {
    est_theta[L_idx_start:L_idx_end] <- vech(Sigma_hat)
  } else {
    est_theta[L_idx_start:L_idx_end] <- diag(Sigma_hat)
  }

  if (!singular_flag && !is.null(vcov_mat)) {
    # successfully computed Hessian inverse
    se <- sqrt(diag(vcov_mat))

    # Jacobian of g : L_params -> vech(Sigma)
    J <- jacobian_vech_Sigma(L_params_hat, K_w, rc_correlation)

    ## Deltaâ€‘method variance & SEs
    vcov_L <- vcov_mat[L_idx_start:L_idx_end, L_idx_start:L_idx_end]
    V_sigma  <- J %*% vcov_L %*% t(J)
    se_sigma <- sqrt(diag(V_sigma))

    ## Make pretty names
    idx_i <- unlist(lapply(seq_len(K_w),    function(i) rep(i, i)))
    idx_j <- unlist(lapply(seq_len(K_w),    function(i) seq_len(i)))
    se_sigma_names <- sprintf("Sigma_%d%d", idx_i, idx_j)

    se[L_idx_start:L_idx_end] <- se_sigma

  } else {
    # either singular Hessian or some inversion error
    message("Standard errors set to NA due to Hessian inversion failure.")
  }

  # Compute z-values and p-values
  zval <- est_theta / se
  # two-sided p-value under normal approximation
  pval <- 2 * (1 - pnorm(abs(zval)))

  # significance codes
  significance_code <- function(p) {
    if (is.na(p)) return("")
    if (p < 0.001) return("***")
    else if (p < 0.01) return("**")
    else if (p < 0.05) return("*")
    else return("")
  }

  # Determine parameter names if not provided
  if (is.null(param_names)) {

    param_names <- character(p_len)

    # Beta parameters
    for (i in seq_len(K_x)) {
      param_names[i] <- paste0("W_", i)
    }

    # L paramters
    for (l in seq_len(L_size)) {
      param_names[K_x + l] <- se_sigma_names[l]
    }

    # ASC parameters (if used)
    if (use_asc && !omit_asc_output) {
      if (delta_length > 0) {
        for (d in seq_len(delta_length)) {
          # If outside option is included, name them delta_1,... else skip 'delta_1'
          if (include_outside_option) {
            param_names[K_x + L_size + d] <- paste0("ASC_", d)
          } else {
            param_names[K_x + L_size + d] <- paste0("ASC_", d + 1)
          }
        }
      } else {
        stop("Not enough parameters in theta for delta, beta, and L.")
      }
    }

  } else {
    # If param_names is provided, ensure it has the correct length
    if (length(param_names) != p_len) {
      stop("param_names must have the same length as the number of parameters.")
    }
  }

  # Build a data frame for results
  #    Add an "Index" column that goes 1..p_len
  res_df <- data.frame(
    Index     = seq_len(p_len),
    Parameter = param_names,
    Estimate  = est_theta,
    Std_Error = se,
    z_value   = zval,
    Pr_z      = pval,
    Signif    = sapply(pval, significance_code),
    stringsAsFactors = FALSE
  )

  # Write CSV
  if(!is.null(file_name)) write.csv(res_df, file_name, row.names = FALSE)

  # Print a formatted table to screen

  # Compute dynamic column widths:
  # - index_colwidth: enough for the largest index
  index_colwidth <- max(nchar(as.character(p_len)), nchar("Index"))

  # - param_colwidth: depends on the longest parameter name
  param_colwidth <- max(nchar(res_df$Parameter), nchar("Parameter"))

  # Helper to print each row
  print_row <- function(x) {
    cat(sprintf(
      # Format: index, param, estimate, std.error, z-value, Pr(>|z|), Signif
      # We'll adjust to the colwidths we computed
      paste0(
        "%", index_colwidth, "d  ",    # Index
        "%-", param_colwidth, "s  ",   # Parameter name (left-justified)
        "%10.6f %10.6f %8.4f %9.2e  %s\n"
      ),
      as.integer(x["Index"]),
      x["Parameter"],
      as.numeric(x["Estimate"]),
      as.numeric(x["Std_Error"]),
      as.numeric(x["z_value"]),
      as.numeric(x["Pr_z"]),
      x["Signif"]
    ))
  }

  # Header row
  cat("Model Coefficients:\n")
  cat(sprintf(
    paste0(
      "%", index_colwidth, "s  ",
      "%-", param_colwidth, "s  ",
      "%10s %10s %8s %9s  %s\n"
    ),
    "Index", "Parameter", "Estimate", "Std.Error", "z-value", "Pr(>|z|)", ""
  ))

  # Print each row
  for (i in seq_len(nrow(res_df))) {
    print_row(res_df[i, ])
  }

  invisible(res_df)
}

remove_nullspace_cols <- function(mat, tol = 1e-7) {
  if (is.null(mat)) return(mat)
  if (ncol(mat)==1) return(mat)
  qrdecomp <- qr(mat, tol = tol)
  rank <- qrdecomp$rank
  if (rank == ncol(mat)) return(mat)
  bad_cols_idx  <- qrdecomp$pivot[(rank + 1):ncol(mat)]
  mat <- mat[, setdiff(1:ncol(mat), bad_cols_idx), drop=FALSE]
  return(mat)
}

check_collinearity <- function(X) {
  colnames_before <- colnames(X)
  X <- remove_nullspace_cols(X)
  colnames_after <- colnames(X)
  colnames_diff <- setdiff(colnames_before, colnames_after)
  if (length(colnames_diff) > 0) {
    cat("The following variables were dropped due to collinearity:\n")
    cat(colnames_diff, "\n")
    vars_drop <- colnames_diff
  }
  return(X)
}
