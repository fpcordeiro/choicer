
#' Runs mixed logit estimation
#'
#' Using inputs generated by `prepare_mxl_data()`, run likelihood maximization for mixed logit.
#'
#' @param input_data List output from `prepare_mxl_data()`
#' - $X: Design matrix
#' - $W: Design matrix for random coefficients
#' - $alt_idx: Alternative indices for each observation
#' - $choice_idx: Chosen alternative indices for each choice situation
#' - $M: Vector of number of alternatives per choice situation
#' - $weights: Vector of weights for each choice situation
#' - $include_outside_option: Logical indicating whether an outside option is included
#' - $rc_correlation: Logical indicating whether random coefficients are correlated
#' - $alt_mapping: Data frame mapping alternatives to nests
#' @param eta_draws Array of shape K_w x S x N with standard normal draws for random coefficients
#' @param rc_dist Integer vector indicating distribution of random coefficients (0=normal, 1=log-normal)
#' @param rc_mean Logical indicating whether to estimate means for random coefficients
#' @param use_asc Logical indicating whether to include alternative-specific constants
#' @param theta_init Initial parameter vector for optimization. If NULL, a zero vector of appropriate length is used.
#' @param nloptr_opts List of options to pass to `nloptr::nloptr()`. If NULL, default options are used.
#' @return Result object from `nloptr::nloptr()`, with an added `alt_mapping` element.
#' @importFrom nloptr nloptr
#' @export
run_mxlogit <- function(
    input_data,
    eta_draws,
    rc_dist = NULL,
    rc_mean = FALSE,
    use_asc = TRUE,
    theta_init = NULL,
    nloptr_opts = NULL
) {

  # Initial parameter vector theta_init
  if (is.null(theta_init)) {
    J <- nrow(input_data$alt_mapping)
    K_x <- ncol(input_data$X)
    K_w <- ncol(input_data$W)
    L_size <- if (input_data$rc_correlation) {K_w * (K_w + 1) / 2} else {K_w}
    mu_size <- if (rc_mean) K_w else 0
    theta_init <- rep(0, K_x + mu_size + L_size + J - 1)
  }

  if (is.null(rc_dist)) rc_dist <- rep(0L, ncol(input_data$W))
  
  # Optimization options (default)
  if (is.null(nloptr_opts)) {
    nloptr_opts <- list(
      "algorithm" = "NLOPT_LD_LBFGS",
      "xtol_rel" = 1.0e-8,
      "maxeval" = 1e+3,
      "print_level" = 0L
    )
  }

  # Run the optimization
  time <- system.time({
    result <- nloptr::nloptr(
      x0 = theta_init,
      eval_f = mxl_loglik_gradient_parallel,
      opts = nloptr_opts,
      X = input_data$X,
      W = input_data$W,
      alt_idx = input_data$alt_idx,
      choice_idx = input_data$choice_idx,
      M = input_data$M,
      weights = input_data$weights,
      rc_dist = rc_dist,
      rc_correlation = input_data$rc_correlation,
      rc_mean = rc_mean,
      eta_draws = eta_draws,
      use_asc = use_asc,
      include_outside_option = input_data$include_outside_option
    )
  })

  cat("Optimization run time", convertTime(time), "\n\n")
  print(result)
  result$alt_mapping <- input_data$alt_mapping
  return(result)
}


#' Prepare inputs for `mxl_loglik_gradient_parallel()`
#'
#' Prepares and validates inputs for mixed logit estimation routine.
#'
#' @param data Data frame containing choice data
#' @param id_col Name of the column identifying choice situations (individuals)
#' @param alt_col Name of the column identifying alternatives
#' @param choice_col Name of the column indicating chosen alternative (1 = chosen, 0 = not chosen)
#' @param covariate_cols Vector of names of columns to be used as covariates
#' @param random_var_cols Vector of names of columns to be used as random variables
#' @param weights Optional vector of weights for each choice situation. If NULL, equal weights are used.
#' @param outside_opt_label Label for the outside option (if any). If NULL, no outside option is assumed.
#' @param include_outside_option Logical indicating whether to include an outside option in the model.
#' @param rc_correlation Logical indicating whether random coefficients are correlated. Default is FALSE.
#' @return A list containing prepared inputs for mixed logit estimation, including rc_correlation.
#' @import data.table
#' @export
prepare_mxl_data <- function(
    data,
    id_col,
    alt_col,
    choice_col,
    covariate_cols,
    random_var_cols,
    weights = NULL,
    outside_opt_label = NULL,
    include_outside_option = FALSE,
    rc_correlation = FALSE
) {

  ## Preliminary housekeeping --------------------------------------------------
  dt <- as.data.table(data)[]

  # Check if all relevant variables are available
  needed <- c(id_col, alt_col, choice_col, covariate_cols, random_var_cols)
  if (!all(needed %in% names(dt)))
    stop("Missing columns: ",
         paste(setdiff(needed, names(dt)), collapse = ", "))

  # Drop non-relevant variables
  vars_to_drop <- setdiff(names(dt), needed)
  if (length(vars_to_drop) > 0) {
    dt[, (vars_to_drop) := NULL]
  }

  ## Drop ids with missing observations ----------------------------------------
  dt[, HAS_NA := rowSums(is.na(.SD)) > 0]
  ids_to_drop <- dt[HAS_NA==TRUE, get(id_col)] |> unique()
  if (length(ids_to_drop) > 0) {
    dt <- dt[!(get(id_col) %in% ids_to_drop)]
    warning("Removed ", length(ids_to_drop),
            " choice situations containing missing values.")
  }
  if (nrow(dt) == 0) {
    stop("All choice situations removed due to missing values.")
  }
  dt[, HAS_NA := NULL]

  ## Sanity checks ---------------------------------------------------------

  ## covariates must be numeric
  if (!all(vapply(dt[, ..covariate_cols], is.numeric, logical(1L))))
    stop("All covariates must be numeric.")
  if (!all(vapply(dt[, ..random_var_cols], is.numeric, logical(1L))))
    stop("All covariates must be numeric.")

  ## choice column must be 0 or 1
  bad_choice <- dt[[choice_col]] %in% c(0, 1) == FALSE
  if (any(bad_choice))
    stop("`", choice_col, "` must contain only 0 and 1.")

  ## Exactly one '1' per choice situation
  by_id <- dt[, .(chosen = sum(get(choice_col))), by = id_col]
  if (include_outside_option == FALSE && any(by_id$chosen != 1)) {
    stop("Each ", id_col, " must have exactly one chosen alternative (one '1' in ",
         choice_col, ").")
  }
  if (include_outside_option && any(by_id$chosen > 1)) {
    stop("Each ", id_col, " must have at most one chosen alternative (one '1' in ",
         choice_col, "). An id with no explicit choice is assumed to be outside option.")
  }

  ## Create integer alternative codes ------------------------------------------

  if (!is.null(outside_opt_label) && include_outside_option==FALSE) {
    levels <- c(outside_opt_label, sort(setdiff(unique(dt[[alt_col]]), outside_opt_label)))
  } else {
    levels <- sort(unique(dt[[alt_col]]))
  }

  dt[, alt_int := as.integer(factor(get(alt_col), levels = levels))]

  ## Order rows ----------------------------------------------------------------
  ##   within each id: ascending alternative id
  ##   between ids   : ascending id
  setorderv(dt, c(id_col, "alt_int"))

  ## index of each row within its choice set
  dt[, idx_in_group := seq_len(.N), by = id_col]

  ## Build objects -------------------------------------------------------------
  ## design matrix
  X <- as.matrix(dt[, ..covariate_cols])
  X_res <- check_collinearity(X)
  X <- X_res$mat
  if (!is.null(X_res$dropped)) dropped_vars <- X_res$dropped # accumulate dropped vars if we had multiple checks


  W <- as.matrix(dt[, ..random_var_cols])
  W_res <- check_collinearity(W)
  W <- W_res$mat
  if (!is.null(W_res$dropped)) {
     if(exists("dropped_vars")) dropped_vars <- c(dropped_vars, W_res$dropped)
     else dropped_vars <- W_res$dropped
  }

  cols_to_drop <- union(covariate_cols, random_var_cols)
  dt[, (cols_to_drop) := NULL]

  ## alternative ids used for delta coefficients
  alt_idx <- as.integer(dt$alt_int)                             # length == sum(M)

  ## M[i] - # alternatives per choice situation
  M <- dt[, .N, by = id_col][["N"]]                             # length N

  ## N: number of individuals / choice situations
  ids <- dt[, get(id_col)][!duplicated(dt[[id_col]])]  # vector of ids in *current* order
  N   <- length(ids)

  ## choice_idx[i] - 1-based index *within* the choice set data
  ## 0 == outside option (only if chosen = 0 for all inside options & include_outside_option == TRUE)
  if (include_outside_option) {
    # start with all-zero (everyone assumed to pick the outside good)
    choice_idx <- integer(N)
    chosen_dt <- dt[get(choice_col) == 1, .(pos = idx_in_group), by = id_col]

    # match chosen ids back to the master index vector
    setkeyv(chosen_dt, id_col)
    choice_idx[match(chosen_dt[[id_col]], ids)] <- chosen_dt$pos
  } else {
    # exactly one explicit choice per id
    choice_idx <- dt[get(choice_col) == 1, idx_in_group]
  }

  # Weights default = 1
  if (is.null(weights)) weights <- rep(1, N)

  ## Alternative summary -------------------------------------------------------

  if (include_outside_option) {
    inside_alt_mapping <- dt[
      , .(N_OBS = .N, N_CHOICES = sum(get(choice_col))),
      keyby = c("alt_int", alt_col)
    ]
    outside_alt_mapping <- data.table(alt_int=0L, N_OBS = N, N_CHOICES = sum(choice_idx == 0L))
    outside_alt_mapping[[alt_col]] <- outside_opt_label
    alt_mapping <- list(outside_alt_mapping, inside_alt_mapping) |>
      rbindlist(use.names = TRUE, fill = TRUE)
    setcolorder(alt_mapping, c("alt_int", alt_col, "N_OBS", "N_CHOICES"))
  } else {
    alt_mapping <- dt[
      , .(N_OBS = .N, N_CHOICES = sum(get(choice_col))),
      keyby = c("alt_int", alt_col)
    ]
  }

  alt_mapping[, `:=`(
    TAKE_RATE = N_CHOICES / N_OBS,
    MKT_SHARE = N_CHOICES / sum(N_CHOICES)
  )]

  ## Final validity checks -----------------------------------------------------
  stopifnot(
    length(alt_idx)    == nrow(X),
    length(choice_idx) == N,
    length(M)          == N,
    length(weights)    == N,
    all(is.finite(X)),
    all(is.finite(W))
  )

  ## Return output -------------------------------------------------------------
  list(
    X           = X,
    W           = W,
    alt_idx     = alt_idx,
    choice_idx  = as.integer(choice_idx),
    M           = M,
    N           = N,
    weights     = weights,
    include_outside_option = include_outside_option,
    rc_correlation = rc_correlation,
    alt_mapping = alt_mapping[],
    dropped_cols = if(exists("dropped_vars")) dropped_vars else NULL
  )
}

#' Halton draws for mixed logit
#'
#' Create halton normal draws in appropriate format for mixed logit estimation
#'
#' @param S Number of draws for each choice situation
#' @param N number of choice situations
#' @param K_w dimension of random coefficients (number of columns in W matrix)
#' @return K_w x S x N array with halton standard normal draws
#' @importFrom randtoolbox halton
#' @export
get_halton_normals <- function(S, N, K_w) {
  # Generate all needed Halton draws at once
  # We need S * N draws for each of K_w dimensions
  total_draws <- S * N

  # Generate Halton sequence
  # randtoolbox::halton returns a matrix of size total_draws x K_w
  halton_seq <- randtoolbox::halton(n = total_draws, dim = K_w, normal = TRUE)

  # Initialize the eta_draws array
  eta_draws <- array(0, dim = c(K_w, S, N))
  
  # Fill the array
  # The original code used: start_index = (i - 1) * S + 1 for each individual
  # This corresponds to taking chunks of S rows from the halton sequence
  for (i in 1:N) {
     start_row <- (i - 1) * S + 1
     end_row   <- i * S
     
     # halton_seq[start:end, ] is S x K_x
     # we want K_x x S for eta_draws[, , i]
     eta_draws[, , i] <- t(halton_seq[start_row:end_row, , drop=FALSE])
  }

  return(eta_draws)
}

#' Coefficient summary table for mixed logit model
#'
#' Prints and saves coefficient summary table for mixed logit model.
#'
#' @param opt_result Result object from nloptr optimization containing at least `solution` element.
#' @param X Design matrix used in estimation.
#' @param W Design matrix for random coefficients.
#' @param alt_idx Alternative indices for each observation.
#' @param choice_idx Chosen alternative indices for each choice situation.
#' @param M Vector of number of alternatives per choice situation.
#' @param weights Vector of weights for each choice situation.
#' @param eta_draws Array of shape K_w x S x N with standard normal draws for random coefficients.
#' @param rc_dist Integer vector indicating distribution of random coefficients (0 = normal, 1 = log-normal).
#' @param rc_correlation Logical indicating whether a full covariance matrix was estimated for random coefficients.
#' @param rc_mean Logical indicating whether means were estimated for random coefficients.
#' @param use_asc Logical indicating whether ASCs were included in the model.
#' @param include_outside_option Logical indicating whether an outside option was included in the model.
#' @param omit_asc_output Logical indicating whether to omit ASC parameters from the output table.
#' @param param_names Optional vector of parameter names. If `NULL`, default names are generated.
#' @param file_name Optional file path to save the coefficient summary table as a CSV file. If `NULL`, no file is saved.
#' @returns A data frame (invisibly) with columns: Index, Parameter, Estimate,
#'   Std_Error, z_value, Pr_z, and Signif. For variance parameters, the delta method
#'   is applied to transform from Cholesky to covariance scale. The table is also
#'   printed to the console.
#' @export
get_mxl_result <- function(
    opt_result,
    X, W, alt_idx, choice_idx, M, weights, eta_draws,
    rc_dist,rc_correlation,rc_mean,
    use_asc,
    include_outside_option,
    omit_asc_output = FALSE,
    param_names = NULL,
    file_name = NULL
) {
  # Extract the estimated parameter vector
  if (is.null(opt_result$solution)) {
    stop("opt_result must contain '$solution' with the parameter estimates.")
  }
  est_theta <- opt_result$solution
  p_len <- length(est_theta)

  K_x <- ncol(X)
  K_w <- ncol(W)
  L_size <- if (rc_correlation) K_w * (K_w + 1) / 2 else K_w
  mu_size <- if (rc_mean) K_w else 0
  delta_length <- length(est_theta) - K_x - mu_size - L_size

  # Define Block Indices
  idx_beta_start <- 1
  idx_beta_end   <- K_x
  idx_mu_start   <- K_x + 1
  idx_mu_end     <- K_x + mu_size
  idx_L_start    <- idx_mu_end + 1
  idx_L_end      <- idx_mu_end + L_size
  idx_delta_start <- idx_L_end + 1

  # Generate sigma parameter names (needed for output regardless of Hessian success)
  # Column-Major Lower Triangular Order: (1,1), (2,1), (2,2), (3,1), (3,2), (3,3), ...
  # This matches the extraction order in C++ vech() and build_var_mat()
  if (L_size > 0) {
    idx_i <- integer(0)
    idx_j <- integer(0)
    for (j in 1:K_w) {
      for (i in j:K_w) {
        idx_i <- c(idx_i, i)
        idx_j <- c(idx_j, j)
      }
    }
    se_sigma_names <- sprintf("Sigma_%d%d", idx_i, idx_j)
  } else {
    se_sigma_names <- character(0)
  }

  # Compute Hessian at est_theta
  hess <- mxl_hessian_parallel(
    theta = est_theta,
    X = X,
    W = W,
    alt_idx = alt_idx,
    choice_idx = choice_idx,
    M = M,
    weights = weights,
    eta_draws = eta_draws,
    rc_dist = rc_dist,
    rc_correlation = rc_correlation,
    rc_mean = rc_mean,
    use_asc = use_asc,
    include_outside_option = include_outside_option
  )

  # Try to invert Hessian for standard errors
  vcov_mat <- NULL
  se <- rep(NA, p_len)  # default in case of errors

  singular_flag <- FALSE
  tryCatch({
    vcov_mat <- solve(hess)
  }, error = function(e) {
    singular_flag <<- TRUE
    message("Error computing vcov_mat (likely singular Hessian): ", e$message)
  })

  if (!singular_flag && !is.null(vcov_mat)) {
    # successfully computed Hessian inverse
    se <- sqrt(diag(vcov_mat))

    # Delta Method for Mean parameters (Mu)
    if (rc_mean) {
      # Loop through Random Coefficients
      for (k in 1:K_w) {
        # Check if this specific coefficient is Log-Normal
        if (rc_dist[k] == 1) { 
          # Current index in the theta vector
          curr_idx <- idx_mu_start + (k - 1)
          
          # Get raw estimates
          mu_hat <- est_theta[curr_idx]
          mu_se  <- se[curr_idx]
          
          # Apply Transformation: exp(mu)
          est_theta[curr_idx] <- exp(mu_hat)
          
          # Apply Delta Method for SE: 
          # SE(exp(mu)) = |d/dmu exp(mu)| * SE(mu) = exp(mu) * SE(mu)
          se[curr_idx] <- exp(mu_hat) * mu_se
        }
      }
    }

    # Delta Method for Variance parameters (L -> Sigma)
    if (L_size > 0) {
      # Only run if we actually have random coefficients
      L_params_hat <- est_theta[idx_L_start:idx_L_end]
      
      # Convert L parameters to Sigma matrix for display
      Sigma_hat <- build_var_mat(L_params_hat, K_w, rc_correlation)
      
      # Replace L in est_theta with Sigma elements
      if (rc_correlation) {
        # vech() extracts lower triangular part
        est_theta[idx_L_start:idx_L_end] <- vech(Sigma_hat)
      } else {
        est_theta[idx_L_start:idx_L_end] <- diag(Sigma_hat)
      }
      
      # Jacobian of g : L_params -> vech(Sigma)
      J <- jacobian_vech_Sigma(L_params_hat, K_w, rc_correlation)
      
      # Extract relevant block from full V-Cov
      vcov_L <- vcov_mat[idx_L_start:idx_L_end, idx_L_start:idx_L_end]
      
      # V_sigma = J * V_L * J'
      V_sigma  <- J %*% vcov_L %*% t(J)
      se_sigma <- sqrt(diag(V_sigma))
      
      se[idx_L_start:idx_L_end] <- se_sigma
    }

  } else {
    # either singular Hessian or some inversion error
    message("Standard errors set to NA due to Hessian inversion failure.")
  }

  # Compute z-values and p-values
  zval <- est_theta / se
  # two-sided p-value under normal approximation
  pval <- 2 * (1 - pnorm(abs(zval)))

  # significance codes
  significance_code <- function(p) {
    if (is.na(p)) return("")
    if (p < 0.001) return("***")
    else if (p < 0.01) return("**")
    else if (p < 0.05) return("*")
    else return("")
  }

  # Determine parameter names if not provided
  if (is.null(param_names)) {

    param_names <- character(p_len)

    # Beta parameters
    for (i in seq_len(K_x)) {
      param_names[idx_beta_start - 1 + i] <- paste0("W_", i)
    }

    # mu parameters
    if (rc_mean) {
      for (i in seq_len(mu_size)) {
        param_names[idx_mu_start - 1 + i] <- paste0("Mu_", i)
      }
    }

    # L paramters
    for (l in seq_len(L_size)) {
      param_names[idx_L_start - 1 + l] <- se_sigma_names[l]
    }

    # ASC parameters (if used)
    if (use_asc && !omit_asc_output) {
      if (delta_length > 0) {
        for (d in seq_len(delta_length)) {
          # If outside option is included, name them delta_1,... else skip 'delta_1'
          if (include_outside_option) {
            param_names[idx_delta_start - 1 + d] <- paste0("ASC_", d)
          } else {
            param_names[idx_delta_start - 1 + d] <- paste0("ASC_", d + 1)
          }
        }
      } else {
        stop("Not enough parameters in theta for delta, beta, and L.")
      }
    }

  } else {
    # If param_names is provided, ensure it has the correct length
    if (length(param_names) != p_len) {
      stop("param_names must have the same length as the number of parameters.")
    }
  }

  # Build a data frame for results
  #    Add an "Index" column that goes 1..p_len
  res_df <- data.frame(
    Index     = seq_len(p_len),
    Parameter = param_names,
    Estimate  = est_theta,
    Std_Error = se,
    z_value   = zval,
    Pr_z      = pval,
    Signif    = sapply(pval, significance_code),
    stringsAsFactors = FALSE
  )

  # Write CSV
  if(!is.null(file_name)) utils::write.csv(res_df, file_name, row.names = FALSE)

  # Print a formatted table to screen

  # Compute dynamic column widths:
  # - index_colwidth: enough for the largest index
  index_colwidth <- max(nchar(as.character(p_len)), nchar("Index"))

  # - param_colwidth: depends on the longest parameter name
  param_colwidth <- max(nchar(res_df$Parameter), nchar("Parameter"))

  # Helper to print each row
  print_row <- function(x) {
    cat(sprintf(
      # Format: index, param, estimate, std.error, z-value, Pr(>|z|), Signif
      # We'll adjust to the colwidths we computed
      paste0(
        "%", index_colwidth, "d  ",    # Index
        "%-", param_colwidth, "s  ",   # Parameter name (left-justified)
        "%10.6f %10.6f %8.4f %9.2e  %s\n"
      ),
      as.integer(x["Index"]),
      x["Parameter"],
      as.numeric(x["Estimate"]),
      as.numeric(x["Std_Error"]),
      as.numeric(x["z_value"]),
      as.numeric(x["Pr_z"]),
      x["Signif"]
    ))
  }

  # Header row
  cat("Model Coefficients:\n")
  cat(sprintf(
    paste0(
      "%", index_colwidth, "s  ",
      "%-", param_colwidth, "s  ",
      "%10s %10s %8s %9s  %s\n"
    ),
    "Index", "Parameter", "Estimate", "Std.Error", "z-value", "Pr(>|z|)", ""
  ))

  # Print each row
  for (i in seq_len(nrow(res_df))) {
    print_row(res_df[i, ])
  }

  invisible(res_df)
}

#' Compute aggregate elasticities for mixed logit model
#'
#' Computes the aggregate elasticity matrix (weighted average of individual
#' elasticities) for the Mixed Logit model. The elasticity E(i,j) represents
#' the percentage change in the probability of choosing alternative i when
#' the attribute of alternative j changes by 1%.
#'
#' @param opt_result Result object from `run_mxlogit()` containing `$solution`.
#' @param input_data List output from `prepare_mxl_data()`.
#' @param eta_draws Array of shape K_w x S x N with standard normal draws.
#' @param rc_dist Integer vector indicating distribution (0 = normal, 1 = log-normal).
#' @param elast_var_idx 1-based index of variable for elasticity computation.
#'   If `is_random_coef = FALSE`, this indexes into X columns. If `is_random_coef = TRUE`,
#'   this indexes into W columns.
#' @param is_random_coef Logical. `TRUE` if the variable has a random coefficient (is in W),
#'   `FALSE` if it has a fixed coefficient (is in X). Default is `FALSE`.
#' @param rc_mean Logical indicating whether means were estimated for random coefficients.
#' @param use_asc Logical indicating whether ASCs were included in the model.
#' @returns A J x J elasticity matrix where entry (i, j) is the elasticity of P_i
#'   with respect to the attribute of alternative j. Row and column names are set
#'   from the alternative labels if available.
#' @export
mxl_elasticities <- function(
    opt_result,
    input_data,
    eta_draws,
    rc_dist,
    elast_var_idx,
    is_random_coef = FALSE,
    rc_mean = FALSE,
    use_asc = TRUE
) {
  if (is.null(opt_result$solution)) {
    stop("opt_result must contain '$solution' with the parameter estimates.")
  }

  elas_mat <- mxl_elasticities_parallel(
    theta = opt_result$solution,
    X = input_data$X,
    W = input_data$W,
    alt_idx = input_data$alt_idx,
    choice_idx = input_data$choice_idx,
    M = input_data$M,
    weights = input_data$weights,
    eta_draws = eta_draws,
    rc_dist = rc_dist,
    elast_var_idx = elast_var_idx,
    is_random_coef = is_random_coef,
    rc_correlation = input_data$rc_correlation,
    rc_mean = rc_mean,
    use_asc = use_asc,
    include_outside_option = input_data$include_outside_option
  )

  # Add row and column names from alt_mapping if available
  if (!is.null(input_data$alt_mapping)) {
    alt_labels <- input_data$alt_mapping[[2]]  # Second column has alternative labels
    if (length(alt_labels) == nrow(elas_mat)) {
      rownames(elas_mat) <- alt_labels
      colnames(elas_mat) <- alt_labels
    }
  }

  return(elas_mat)
}

#' BLP contraction mapping for mixed logit
#'
#' Finds the ASC (delta) parameters such that predicted market shares
#' match target shares, using the contraction mapping of Berry, Levinsohn,
#' and Pakes (1995). This is useful for demand estimation and counterfactual
#' simulations.
#'
#' @param delta_init Initial guess for delta (ASC) values. Should be J-1 elements
#'   if `include_outside_option = FALSE`, or J elements if `include_outside_option = TRUE`.
#' @param target_shares Target market shares. Must have J elements (including
#'   outside option if applicable).
#' @param input_data List output from `prepare_mxl_data()`.
#' @param beta Fixed coefficients vector (K_x elements).
#' @param mu Mean parameters for random coefficients (K_w elements). Use zeros
#'   if `rc_mean = FALSE`.
#' @param L_params Cholesky parameters vector. Length is K_w * (K_w + 1) / 2 if
#'   `rc_correlation = TRUE`, or K_w if `rc_correlation = FALSE`.
#' @param eta_draws Array of shape K_w x S x N with standard normal draws.
#' @param rc_dist Integer vector indicating distribution (0 = normal, 1 = log-normal).
#' @param rc_mean Logical indicating whether means are estimated for random coefficients.
#' @param tol Convergence tolerance. Default is 1e-8.
#' @param max_iter Maximum number of iterations. Default is 1000.
#' @returns Converged delta (ASC) vector. Length is J-1 if `include_outside_option = FALSE`,
#'   or J if `include_outside_option = TRUE`.
#' @export
mxl_blp <- function(
    delta_init,
    target_shares,
    input_data,
    beta,
    mu,
    L_params,
    eta_draws,
    rc_dist,
    rc_mean = FALSE,
    tol = 1e-8,
    max_iter = 1000
) {
  delta_out <- mxl_blp_contraction(
    delta = delta_init,
    target_shares = target_shares,
    X = input_data$X,
    W = input_data$W,
    beta = beta,
    mu = mu,
    L_params = L_params,
    alt_idx = input_data$alt_idx,
    M = input_data$M,
    weights = input_data$weights,
    eta_draws = eta_draws,
    rc_dist = rc_dist,
    rc_correlation = input_data$rc_correlation,
    rc_mean = rc_mean,
    include_outside_option = input_data$include_outside_option,
    tol = tol,
    max_iter = max_iter
  )

  return(delta_out)
}
